{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a561a4ba",
   "metadata": {},
   "source": [
    "## Chat with PyTorch GPT2 Model\n",
    "Create a GPT-2 model as per Sebastian Raschka's \"Build a Large Language Model From Scratch\". Load its parameters from the open source weights released by Open AI, then test with a few iterations of chat-like interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f71343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "os.sys.path.append(r\"C:\\\\Code\\\\SDev.Python\") # Path to root above sdevpy\n",
    "from sdevpy.machinelearning.llms import gpt\n",
    "from sdevpy.machinelearning.llms import textgen as tg\n",
    "\n",
    "# Global setup\n",
    "model_source_folder = r\"C:\\\\SDev.Finance\\\\OneDrive\\\\LLM\\\\models\\\\gpt2\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ce059",
   "metadata": {},
   "source": [
    "### Load pre-trained model\n",
    "We have saved a number of parameter sets for various sizes of GPT2 models, released open source by OpenAI. Pick the model size, then create the model and load the parameters in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95d8b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for GPT-2 model size: 124M\n",
      "Done loading weights!\n"
     ]
    }
   ],
   "source": [
    "model_size = \"124M\" # 124M, 355M, 774M, 1558M\n",
    "\n",
    "# Retrieve parameters\n",
    "model_folder = os.path.join(model_source_folder, model_size)\n",
    "settings, params = gpt.load_gpt2(model_folder)\n",
    "# print(\"Settings: \", settings)\n",
    "# print(\"Param dict keys: \", params.keys())\n",
    "\n",
    "# Create model\n",
    "GPT_CONFIG = {\"vocab_size\": settings['n_vocab'], \"context_length\": settings['n_ctx'],\n",
    "              \"emb_dim\": settings['n_embd'], \"n_heads\": settings['n_head'],\n",
    "              \"n_layers\": settings['n_layer'], \"drop_rate\": 0.1, \"qkv_bias\": True}\n",
    "context_length = GPT_CONFIG[\"context_length\"]\n",
    "\n",
    "model = gpt.GPTModel(GPT_CONFIG)\n",
    "model.eval(); # Skip printing model details\n",
    "\n",
    "# Load parameters into model\n",
    "print(\"Loading weights for GPT-2 model size: \" + model_size)\n",
    "gpt.load_weights(model, params)\n",
    "print(\"Done loading weights!\")\n",
    "\n",
    "# Send model to device\n",
    "model.to(device); # Skip printing model details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebfcf71",
   "metadata": {},
   "source": [
    "### Chat with the model\n",
    "Start by picking the next token generation model and creating a ChatGenerator to handle multi-step dialogs. Then chat with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773448e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "token_gen = tg.NextTokenGenerator(top_k=15, temperature=1.5)\n",
    "chat_gen = tg.ChatGenerator(device, model, tokenizer, context_length, token_gen,\n",
    "                            max_new_tokens=50, max_sentences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6deb51bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " A pie crust I'm going to bake? A cake or a cookie?\n"
     ]
    }
   ],
   "source": [
    "# First iteration\n",
    "start_text = \"What do I need to bake an apple cake?\"\n",
    "end_text = chat_gen.end_text(start_text)\n",
    "print(\"Output text:\\n\", tg.format_answer(start_text, end_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9b063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " If I don't want my cakes to get stuck in mud then I should make this with aluminum. This would be a very simple process and would require some skill.\n"
     ]
    }
   ],
   "source": [
    "# Generic iteration\n",
    "new_text = \"Should I put arsenic in the dough?\"\n",
    "start_text = end_text + \"\\n\" + new_text\n",
    "end_text = chat_gen.end_text(start_text)\n",
    "print(\"Output text:\\n\", tg.format_answer(start_text, end_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a9089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311.tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
