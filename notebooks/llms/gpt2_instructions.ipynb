{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a561a4ba",
   "metadata": {},
   "source": [
    "## GPT2 Models for Json instructions\n",
    "* Develop script for json instruction on raw GPT2 model\n",
    "* Develop script for pre-training on json instructions\n",
    "* Develop script for LORA fine-tuning on json instructions\n",
    "* Learn how to download/load Llama, Store Llama in OneDrive\n",
    "* Test Llama on json instruction\n",
    "* Test fine-tuning Llama on json instruction\n",
    "* Test code on GPU-enabled platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f71343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "os.sys.path.append(r\"C:\\\\Code\\\\SDev.Python\") # Path to root above sdevpy\n",
    "from sdevpy.llms import gpt\n",
    "from sdevpy.llms import textgen as tg\n",
    "\n",
    "# Global setup\n",
    "model_source_folder = r\"C:\\\\SDev.Finance\\\\OneDrive\\\\LLM\\\\models\\\\gpt2\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ce059",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95d8b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for GPT-2 model size: 355M\n",
      "Done loading weights!\n"
     ]
    }
   ],
   "source": [
    "model_size = \"355M\" # 124M, 355M, 774M, 1558M\n",
    "\n",
    "# Retrieve parameters\n",
    "model_folder = os.path.join(model_source_folder, model_size)\n",
    "settings, params = gpt.load_gpt2(model_folder)\n",
    "# print(\"Settings: \", settings)\n",
    "# print(\"Param dict keys: \", params.keys())\n",
    "\n",
    "# Create model\n",
    "GPT_CONFIG = {\"vocab_size\": settings['n_vocab'], \"context_length\": settings['n_ctx'],\n",
    "              \"emb_dim\": settings['n_embd'], \"n_heads\": settings['n_head'],\n",
    "              \"n_layers\": settings['n_layer'], \"drop_rate\": 0.1, \"qkv_bias\": True}\n",
    "context_length = GPT_CONFIG[\"context_length\"]\n",
    "\n",
    "model = gpt.GPTModel(GPT_CONFIG)\n",
    "model.eval(); # Skip printing model details\n",
    "\n",
    "# Load parameters into model\n",
    "print(\"Loading weights for GPT-2 model size: \" + model_size)\n",
    "gpt.load_weights(model, params)\n",
    "print(\"Done loading weights!\")\n",
    "\n",
    "# Send model to device\n",
    "model.to(device); # Skip printing model details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebfcf71",
   "metadata": {},
   "source": [
    "Check the model works by chatting with it. Choose token generator and initialize the ChatGenerator object, then use it to have a dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773448e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "token_gen = tg.NextTokenGenerator(top_k=15, temperature=1.5)\n",
    "chat_gen = tg.ChatGenerator(device, model, tokenizer, context_length, token_gen,\n",
    "                            max_new_tokens=50, max_sentences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6deb51bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      " Why is the President an idiot?\n",
      "\n",
      "Output text:\n",
      " Why did he get elected to take us to War? Why are you fighting?\n"
     ]
    }
   ],
   "source": [
    "start_text = \"Why is the President an idiot?\"\n",
    "print(\"Input text:\\n\", start_text)\n",
    "print()\n",
    "end_text = chat_gen.end_text(start_text)\n",
    "print(\"Output text:\\n\", tg.format_answer(start_text, end_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text:\n",
      " He didn't get elected to take us to War, did he?\n",
      "\n",
      "Output text:\n",
      " The reason they're not at war now is he didn't get in there with a military, that they didn't need anymore and there are some countries that we don't get into yet. He didn't get in there with a military.\n"
     ]
    }
   ],
   "source": [
    "new_text = \"He didn't get elected to take us to War, did he?\"\n",
    "start_text = end_text + \"\\n\" + new_text\n",
    "print(\"Input text:\\n\", new_text)\n",
    "print()\n",
    "end_text = chat_gen.end_text(start_text)\n",
    "print(\"Output text:\\n\", tg.format_answer(start_text, end_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfee598",
   "metadata": {},
   "source": [
    "Now using instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13e8517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:  {'name': None, 'age': None, 'occupation': None, 'email': None, 'phone': None}\n"
     ]
    }
   ],
   "source": [
    "# Define json schema\n",
    "schema = {\"name\": None, \"age\": None, \"occupation\": None, \"email\": None, \"phone\": None }\n",
    "print(\"Schema: \", schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da308d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311.tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
