{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a561a4ba",
   "metadata": {},
   "source": [
    "## Hugging Face: Inference to Json (Local)\n",
    "\n",
    "Common small models that run efficiently on CPUs:\n",
    "* microsoft/DialoGPT-small (117M, 500MB): basic conversation, very fast on CPU\n",
    "* gpt2 (124M, 500MB): classic, reliable, fast inference\n",
    "* distilgpt2 (82M, 350MB): lighter version of GPT-2, faster than GPT-2\n",
    "\n",
    "Remember to add the environment variable HF_TOKEN containing your token, or login to HF by\n",
    "> huggingface-cli login\n",
    "\n",
    "and give the token.\n",
    "\n",
    "ToDo\n",
    "* Sample to display information about the model (such as number of parameters)\n",
    "* Develop script for Full/LORA fine-tuning on json instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f71343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # For local models\n",
    "import torch\n",
    "import warnings\n",
    "# import logging\n",
    "\n",
    "# Stop the endless flow of nagging\n",
    "warnings.filterwarnings(\"ignore\")#, message=\".*attention mask.*\")\n",
    "# os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# logging.set_verbosity_error()\n",
    "\n",
    "# Global setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device type: \" + device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5e578",
   "metadata": {},
   "source": [
    "### Example for chatting\n",
    "We show how to use local models for the simplest objective of chatting. We use two versions of the code. The bare version that allows more control, and the version with pipelines for easier syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37657230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who wrote the Bible? The Bible is a book of sacred scripture that is written by many different authors, including both Old\n"
     ]
    }
   ],
   "source": [
    "# Single-turn chat through bare interface\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\" # gpt2\n",
    "# model_name = \"distilgpt2\"\n",
    "# model_name = \"microsoft/DialoGPT-small\"\n",
    "# model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Who wrote the Bible?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(input_ids, max_length=25, num_return_sequences=1,\n",
    "                         pad_token_id=tokenizer.eos_token_id, temperature=0.7, do_sample=True)\n",
    "\n",
    "# Decode\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfb33945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Hello! How are you?\n",
      "Bot:  (pause) This is lovely! I haven't seen you for a while but I do like you. (pause) You're always nice to me, you know. (pause) I'm sure you've found a lot of interesting things with your cock. I mean, you've been looking for something like that before, but this is the first time. You've been feeling pretty good. So, I guess, here it is. (pause) I'm just\n",
      "\n",
      "Prompt: What's your favorite color?\n",
      "Bot:  I like it or not. I mean, I'm not sure if I like it or not. I mean, I'm not sure if I like it or not. I mean, I'm not sure if I like it or not. I mean\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn chat through bare interface\n",
    "chat_history_ids = None\n",
    "\n",
    "# First message\n",
    "prompt = \"Hello! How are you?\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "chat_history_ids = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id,\n",
    "                                  temperature=0.7, do_sample=True)\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "print(f\"Bot: {response}\")\n",
    "\n",
    "# Continue conversation\n",
    "print()\n",
    "prompt = \"What's your favorite color?\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "chat_history_ids = torch.cat([chat_history_ids, input_ids], dim=-1)\n",
    "\n",
    "chat_history_ids = model.generate(chat_history_ids, max_length=200, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "response = tokenizer.decode(chat_history_ids[:, -50:][0], skip_special_tokens=True)\n",
    "print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72d6eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: [\"What's your favorite color?\", 'Tell me a joke']\n",
      "Prompt: What's your favorite color?\n",
      "Bot: What's your favorite color? Do you have a favorite color that you like to wear or use in your daily life?\n",
      "I have to say, I'm a big fan of blue. It's such a calming color, and it always makes me feel happy and relaxed. I've been known to wear blue in my daily life, whether it's my shirt or my scarf. I also love how blue looks against a white background - it's just so striking and visually appealing. Have you ever had a favorite color that you've always stuck with, or do you like to mix and match different colors in your life?\n",
      "\n",
      "Prompt: Tell me a joke\n",
      "Bot: Tell me a joke. Why did the chicken go to the doctor?\n",
      "\n",
      "I don't know, why?\n",
      "\n",
      "Because it had fowl breath!\n",
      "\n",
      "I hope that made you cluck with laughter!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat through pipeline\n",
    "chatbot = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "prompt = [\"What's your favorite color?\", \"Tell me a joke\"]\n",
    "\n",
    "# Generate response\n",
    "response = chatbot(prompt, truncation=True, num_return_sequences=1,\n",
    "                   pad_token_id=tokenizer.eos_token_id) # max_length=50\n",
    "\n",
    "for p, r in zip(prompt, response):\n",
    "    print(f\"Prompt: {p}\")\n",
    "    print(f\"Bot: {r[0]['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ce059",
   "metadata": {},
   "source": [
    "### Extract json information from email\n",
    "Define a json schema for the output. Read a sample email in a text file. Give instruction to the model to extract information from the input email in the json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "172f8242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'customer_name': {'title': 'Customer Name', 'type': 'string'}, 'phone_number': {'title': 'Phone Number', 'type': 'string'}, 'order_number': {'title': 'Order Number', 'type': 'string'}, 'delivery_address': {'title': 'Delivery Address', 'type': 'string'}}, 'required': ['customer_name', 'phone_number', 'order_number', 'delivery_address'], 'title': 'OutputSchemaModel', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "# Define json schema\n",
    "class OutputSchemaModel(BaseModel):\n",
    "    customer_name: str\n",
    "    phone_number: str\n",
    "    order_number: str\n",
    "    delivery_address: str\n",
    "\n",
    "output_schema = OutputSchemaModel.model_json_schema()\n",
    "print(output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "048f6f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Issue with Recent Order #48291\n",
      "\n",
      "From: emma.johnson@example.com\n",
      "\n",
      "To: support@shopfast.com\n",
      "\n",
      "Date: October 26, 2025\n",
      "\n",
      "Hi ShopFast team,\n",
      "\n",
      "I placed an order (Order #48291) on October 20, but the package hasn’t arrived yet at 456 Kennedy Ave, 121489 Atlanta, even though the tracking page says “Delivered” since October 23. Could you please check what happened?\n",
      "\n",
      "Also, I was charged twice for this order on my credit card. Please confirm if I’ll get a refund for the duplicate charge.\n",
      "\n",
      "Thanks,\n",
      "Emma Johnson\n",
      "+44 7911 123456\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(data_folder, \"customer_support.txt\")\n",
    "with open(file, \"r\", encoding='utf-8') as f:\n",
    "    email = f.read()\n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae8620",
   "metadata": {},
   "source": [
    "The first time AutoModelForCausalLM.from_pretrained() is called, it will download the model to the local drive, typically under C:\\Users\\YourUserName\\.cache\\huggingface\\hub\\."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95d8b8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Downloads model to your computer (one-time download)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should show cuda or cpu\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "# model_name = \"google/flan-t5-base\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Downloads model to your computer (one-time download)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Device: {model.device}\")  # Should show cuda or cpu\n",
    "print(f\"Parameters in memory: {model.num_parameters():,}\")\n",
    "# print(f\"Memory usage: {torch.cpu.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Hello and welcome to my home, I'm so glad you could join me for a cup of tea and\n"
     ]
    }
   ],
   "source": [
    "# Chat using direct call\n",
    "inputs = tokenizer(\"Hello\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "result = tokenizer.decode(outputs[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c69a7f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Hey how are you doing today? I'm doing well, thanks! I was just thinking about my old college days and how much I loved learning about different types of plants and flowers. I'm sure you're aware that there are a lot of species that are just as beautiful and unique as any other, but there are also some that are more...well, let's just say they're not so great.\\n\\nI think one of the things that really stood out to me about my college days was the diversity of plant species. There were so many different types of flowers, trees, and shrubs that I never would have seen otherwise. It was amazing to see how different the world was to me, and how much I was learning about the natural world.\\n\\nI'm sure you have a similar experience, don't you? I mean, I'm sure you've had your own experiences of learning about different species and plants, and seeing how unique and beautiful they are. Am I right?\\n\\nAnd I have to ask, what do you think is the most interesting or unique plant species that you've come across? I'm always looking for new and exciting things to learn about, and I'd love to hear about your experiences.\\n\\nThanks for chatting with me! I'm really enjoying this conversation.\"}]\n"
     ]
    }
   ],
   "source": [
    "# Chat using pipeline\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model_name)\n",
    "response = pipeline(\"Hey how are you doing today?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dcad8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json inference\n",
    "def extract_json(text):\n",
    "    prompt = f\"Extract as JSON: {text}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "340ffbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract as JSON: John Doe is 30 years old, email john@example.com, and has a degree in Computer Science. John is married to Jane Doe, who is 28 years old, has a degree in Business, and has a degree in Psychology. John and Jane are both married and have two children, Emily (10) and Michael (7). John has a job at XYZ Corporation, which is a large and well-established company. The company has over 1,000 employees and is headquartered in New York City. John's salary is $120,000 per year. He is also a member of the New York City Police Department, which is responsible for maintaining law and order in the city. John is a member of the local community center and participates in the annual charity event for children's health and education. John has a car and drives a Honda Civic. He enjoys playing basketball and hiking in his free time. John is a big fan of the New York Yankees and attends their games whenever he can. He is a big fan of the New England Patriots and attends their games whenever he can. John is a member of the local Lions Club and participates in the annual charity event for children's health and education. John has a car and drives a Honda Civic. He enjoys playing basketball and hiking in his free time.\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Works offline!\n",
    "result = extract_json(\"John Doe is 30 years old, email john@example.com\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250ae43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311.tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
