{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a561a4ba",
   "metadata": {},
   "source": [
    "## Hugging Face: Inference to Json (Local)\n",
    "\n",
    "Common small models that run efficiently on CPUs:\n",
    "* microsoft/DialoGPT-small (117M, 500MB): basic conversation, very fast on CPU\n",
    "* gpt2 (124M, 500MB): classic, reliable, fast inference\n",
    "* distilgpt2 (82M, 350MB): lighter version of GPT-2, faster than GPT-2\n",
    "\n",
    "Remember to add the environment variable HF_TOKEN containing your token, or login to HF by\n",
    "> huggingface-cli login\n",
    "\n",
    "and give the token.\n",
    "\n",
    "ToDo\n",
    "* Sample to display information about the model (such as number of parameters)\n",
    "* Develop script for Full/LORA fine-tuning on json instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05f71343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.10\n",
      "Transformers: 4.57.1\n",
      "PyTorch: 2.9.0+cpu\n",
      "Device type: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import transformers, torch, json, warnings # logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Stop the endless nagging\n",
    "warnings.filterwarnings(\"ignore\")#, message=\".*attention mask.*\")\n",
    "# os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# logging.set_verbosity_error()\n",
    "\n",
    "print(f\"Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "# Global setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device type: \" + device.type)\n",
    "root_folder = r\"C:\\\\temp\\\\llms\"\n",
    "data_folder = os.path.join(root_folder, \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc2210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: meta-llama/Llama-3.2-1B-Instruct\n",
      "Number of parameters: 1,235,814,400\n",
      "Running on: cpu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"distilgpt2\"\n",
    "# model_name = \"microsoft/DialoGPT-small\"\n",
    "# model_name = \"gpt2\"\n",
    "# model_name = \"google/flan-t5-base\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Retrieve local model (download on first call)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Running on: {model.device}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f49e3",
   "metadata": {},
   "source": [
    "### Examples for chatting\n",
    "We show how to use local models for the simplest objective of chatting. We use two versions of the code. The bare version that allows more control, and the version with pipelines for easier syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb33945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: meta-llama/Llama-3.2-1B-Instruct\n",
      "Number of parameters: 1,235,814,400\n",
      "Running on: cpu \n",
      "\n",
      "Prompt: Hello! How are you?\n",
      "Bot:  I'm excited to start this new project with you! I've been thinking about how we can improve the user experience on our website.\n",
      "\n",
      "I'd like to propose a few ideas to enhance the user experience. Firstly, I think we could create a more intuitive navigation menu that makes it easier for users to find what they need. Perhaps we could add a section for frequently asked questions or a \"help\" section that provides clear instructions on how to use our website.\n",
      "\n",
      "Additionally\n",
      "\n",
      "Prompt: What's your favorite color?\n",
      "Bot:  I was thinking of implementing a new feature that would allow users to save their favorite pages or articles for later. This could be a \" favorites\" section that allows users to save content for easy access later. Do you think this is something that would be\n"
     ]
    }
   ],
   "source": [
    "# Initialize chat history\n",
    "chat_history_ids = None\n",
    "\n",
    "# First message\n",
    "prompt = \"Hello! How are you?\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "chat_history_ids = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id,\n",
    "                                  temperature=0.7, do_sample=True)\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "print(f\"Bot: {response}\")\n",
    "\n",
    "# Continue conversation\n",
    "print()\n",
    "prompt = \"What's your favorite color?\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "chat_history_ids = torch.cat([chat_history_ids, input_ids], dim=-1)\n",
    "\n",
    "chat_history_ids = model.generate(chat_history_ids, max_length=200, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "response = tokenizer.decode(chat_history_ids[:, -50:][0], skip_special_tokens=True)\n",
    "print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72d6eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What's your favorite color?\n",
      "Bot:  What's your favorite food? What's your favorite hobby? What's your favorite place to visit?\n",
      "Do you have any pets? What's your favorite type of music?\n",
      "\n",
      "I'd love to hear about your interests and preferences! It's always great to meet someone who shares similar tastes and passions.\n",
      "\n",
      "Also, I have to ask: Are you a morning person, a night owl, or somewhere in between?\n",
      "\n",
      "Prompt: Tell me a joke\n",
      "Bot: . Why was the math book sad?\n",
      "\n",
      "(wait for the punchline)\n",
      "\n",
      "Because it had too many problems!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat through pipeline\n",
    "chatbot = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "prompt = [\"What's your favorite color?\", \"Tell me a joke\"]\n",
    "\n",
    "# Generate response\n",
    "response = chatbot(prompt, truncation=True, num_return_sequences=1,\n",
    "                   pad_token_id=tokenizer.eos_token_id) # max_length=50\n",
    "\n",
    "for p, r in zip(prompt, response):\n",
    "    print(f\"Prompt: {p}\")\n",
    "    print(f\"Bot: {r[0]['generated_text'][len(p):]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ce059",
   "metadata": {},
   "source": [
    "### Extract json information\n",
    "Define a json schema for the output. This is done by using the package Pydantic, which allows to define an object and have it translated into a json schema. It is then that schema that is passed to the model through the prompt.\n",
    "\n",
    "The data is a sample email in text. We give instruction to the model to extract information from the input email in the json format. Models suitable for instruction have been trained with a specific instruction syntax, which may differ with the model and should be followed for optimal response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f8242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'customer_name': {'title': 'Customer Name', 'type': 'string'}, 'phone_number': {'title': 'Phone Number', 'type': 'string'}, 'order_number': {'title': 'Order Number', 'type': 'string'}, 'delivery_address': {'title': 'Delivery Address', 'type': 'string'}}, 'required': ['customer_name', 'phone_number', 'order_number', 'delivery_address'], 'title': 'OutputSchemaModel', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "# Define pydantic object out of which the json schema is extracted\n",
    "class OutputSchemaModel(BaseModel):\n",
    "    customer_name: str\n",
    "    phone_number: str\n",
    "    order_number: str\n",
    "    delivery_address: str\n",
    "\n",
    "schema = OutputSchemaModel.model_json_schema()\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "048f6f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: URGENT: Non-Delivery of Order #9876543 - Expected Delivery Date 2025-11-01\n",
      "\n",
      "Dear ElecTools, Customer Support Team,\n",
      "\n",
      "I am writing to inquire about the status of my order, #9876543, which was placed on October 25, 2025.\n",
      "The item, an Astro-Fit Backpack, was scheduled for delivery yesterday, November 1, 2025.\n",
      "I have checked the tracking link multiple times (Tracking ID: 1Z99999999), but the status has not updated\n",
      "in over 72 hours and still shows the package as being in the initial shipping facility.\n",
      "\n",
      "I need an immediate update on where my package is and confirmation of a new, guaranteed delivery date.\n",
      "If you cannot provide a confirmed delivery date within the next two business days, I request a full refund\n",
      "for the purchase price. Please treat this issue with urgency.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Jason Bourne,\n",
      "3 Rue de la Paix,\n",
      "83136 Gareoult, France\n",
      "Tel: +33 874 7373 738\n",
      "Email: jason.bourne@cia.com \n"
     ]
    }
   ],
   "source": [
    "email = \"\"\"Subject: URGENT: Non-Delivery of Order #9876543 - Expected Delivery Date 2025-11-01\n",
    "\n",
    "Dear ElecTools, Customer Support Team,\n",
    "\n",
    "I am writing to inquire about the status of my order, #9876543, which was placed on October 25, 2025.\n",
    "The item, an Astro-Fit Backpack, was scheduled for delivery yesterday, November 1, 2025.\n",
    "I have checked the tracking link multiple times (Tracking ID: 1Z99999999), but the status has not updated\n",
    "in over 72 hours and still shows the package as being in the initial shipping facility.\n",
    "\n",
    "I need an immediate update on where my package is and confirmation of a new, guaranteed delivery date.\n",
    "If you cannot provide a confirmed delivery date within the next two business days, I request a full refund\n",
    "for the purchase price. Please treat this issue with urgency.\n",
    "\n",
    "Sincerely,\n",
    "\n",
    "Jason Bourne,\n",
    "3 Rue de la Paix,\n",
    "83136 Gareoult, France\n",
    "Tel: +33 874 7373 738\n",
    "Email: jason.bourne@cia.com \"\"\"\n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae8620",
   "metadata": {},
   "source": [
    "Create a prompt following the Llama template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dcad8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the prompt using Llama 3.2 chat template\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that extracts information from emails and returns valid JSON. Only return the JSON object, no additional text or explanation.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Extract the following information from the email below and return it as a JSON object matching this schema:\n",
    "\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "Email:\n",
    "{email}\n",
    "\n",
    "Return only the JSON object:\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "340ffbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:\n",
      "{\n",
      "  \"customer_name\": \"Jason Bourne\",\n",
      "  \"phone_number\": \"+33 874 7373 738\",\n",
      "  \"order_number\": \"9876543\",\n",
      "  \"delivery_address\": \"3 Rue de la Paix, 83136 Gareoult, France\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Generate model response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512,\n",
    "                             temperature=0.1, # Low temperature for more deterministic output\n",
    "                             do_sample=True, top_p=0.9,\n",
    "                             pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "print(\"Model Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a250ae43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON:\n",
      "{\n",
      "  \"customer_name\": \"Jason Bourne\",\n",
      "  \"phone_number\": \"+33 874 7373 738\",\n",
      "  \"order_number\": \"9876543\",\n",
      "  \"delivery_address\": \"3 Rue de la Paix, 83136 Gareoult, France\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check the output json\n",
    "try:\n",
    "    # Clean up response if needed\n",
    "    json_start = response.find('{')\n",
    "    json_end = response.rfind('}') + 1\n",
    "    if json_start != -1 and json_end > json_start:\n",
    "        json_str = response[json_start:json_end]\n",
    "        parsed_json = json.loads(json_str)\n",
    "        print(\"Parsed JSON:\")\n",
    "        print(json.dumps(parsed_json, indent=2))\n",
    "    else:\n",
    "        print(\"Could not find valid JSON in response\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"SON parsing error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e8843",
   "metadata": {},
   "source": [
    "### Extract trade information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60ccd381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'Counterparty': {'title': 'Counterparty', 'type': 'string'}, 'Notional': {'title': 'Notional', 'type': 'string'}, 'BuySell': {'title': 'Buysell', 'type': 'string'}, 'Maturity': {'title': 'Maturity', 'type': 'string'}, 'Index': {'title': 'Index', 'type': 'string'}, 'CallPut': {'title': 'Callput', 'type': 'string'}, 'Strike': {'title': 'Strike', 'type': 'number'}, 'BarrierLevel': {'title': 'Barrierlevel', 'type': 'number'}, 'UpDown': {'title': 'Updown', 'type': 'string'}}, 'required': ['Counterparty', 'Notional', 'BuySell', 'Maturity', 'Index', 'CallPut', 'Strike', 'BarrierLevel', 'UpDown'], 'title': 'TradeSchemaModel', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "# Define pydantic object out of which the json schema is extracted\n",
    "class TradeSchemaModel(BaseModel):\n",
    "    Counterparty: str\n",
    "    Notional: str\n",
    "    BuySell: str\n",
    "    Maturity: str\n",
    "    Index: str\n",
    "    CallPut: str\n",
    "    Strike: float\n",
    "    BarrierLevel: float\n",
    "    UpDown: str\n",
    "\n",
    "schema = TradeSchemaModel.model_json_schema()\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9440a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counterparty Alpha Inc. Ltd. selling 10000 units of call option with strike 120 on index SPX,\n",
      "expiring on November 15th, 2036. If the spot level goes above 140, the trade shall redeem at 0 cost.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "term_sheet = \"\"\"\n",
    "Counterparty Alpha Inc. Ltd. selling 10000 units of call option with strike 120 on index SPX,\n",
    "expiring on November 15th, 2036. If the spot level goes above 140, the trade shall redeem at 0 cost.\n",
    "\"\"\"\n",
    "\n",
    "print(term_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "508a0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the prompt using Llama 3.2 chat template\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a financial institution agent that extracts information from financial term sheets and returns valid JSON. Only return the JSON object, no additional text or explanation.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Extract the following information from the term sheet below.\n",
    "                       Return as a JSON object matching this schema:\n",
    "\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "        The UpDown value should either be 'Up' or 'Down'.\n",
    "\n",
    "Email:\n",
    "{term_sheet}\n",
    "\n",
    "Return only the JSON object:\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade definition:\n",
      "{\n",
      "  \"Counterparty\": \"Alpha Inc. Ltd.\",\n",
      "  \"Notional\": \"10000\",\n",
      "  \"BuySell\": \"Sell\",\n",
      "  \"Maturity\": \"2036-11-15\",\n",
      "  \"Index\": \"SPX\",\n",
      "  \"CallPut\": \"Call\",\n",
      "  \"Strike\": 120,\n",
      "  \"BarrierLevel\": 140,\n",
      "  \"UpDown\": \"Up\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Generate model response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1, do_sample=True,\n",
    "                             top_p=0.9, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode\n",
    "response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "print(\"Trade definition:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe503e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
