{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a561a4ba",
   "metadata": {},
   "source": [
    "## Hugging Face: Inference to Json (Local)\n",
    "\n",
    "Common small models that run efficiently on CPUs:\n",
    "* microsoft/DialoGPT-small (117M, 500MB): basic conversation, very fast on CPU\n",
    "* gpt2 (124M, 500MB): classic, reliable, fast inference\n",
    "* distilgpt2 (82M, 350MB): lighter version of GPT-2, faster than GPT-2\n",
    "\n",
    "Remember to add the environment variable HF_TOKEN containing your token, or login to HF by\n",
    "> huggingface-cli login\n",
    "\n",
    "and give the token.\n",
    "\n",
    "ToDo\n",
    "* Sample to display information about the model (such as number of parameters)\n",
    "* Develop script for Full/LORA fine-tuning on json instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f71343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers, torch, warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # For local models\n",
    "# import torch\n",
    "from pydantic import BaseModel\n",
    "# import warnings\n",
    "# import logging\n",
    "\n",
    "# Stop the endless flow of nagging\n",
    "warnings.filterwarnings(\"ignore\")#, message=\".*attention mask.*\")\n",
    "# os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# logging.set_verbosity_error()\n",
    "\n",
    "# Global setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device type: \" + device.type)\n",
    "root_folder = r\"C:\\\\temp\\\\llms\"\n",
    "data_folder = os.path.join(root_folder, \"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5e578",
   "metadata": {},
   "source": [
    "### Examples for chatting\n",
    "We show how to use local models for the simplest objective of chatting. We use two versions of the code. The bare version that allows more control, and the version with pipelines for easier syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37657230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who wrote the Bible? The Bible is a book of sacred scripture that is written by many different authors, including both Old\n"
     ]
    }
   ],
   "source": [
    "# # Single-turn chat through bare interface\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# # model_name = \"distilgpt2\"\n",
    "# # model_name = \"microsoft/DialoGPT-small\"\n",
    "# # model_name = \"gpt2\"\n",
    "# # model_name = \"google/flan-t5-base\"\n",
    "# # model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# # Retrieve local model (download on first call)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# print(f\"Model name: {model_name}\")\n",
    "# print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "# print(f\"Running on: {model.device}\", \"\\n\")\n",
    "\n",
    "# # Prompt\n",
    "# prompt = \"Who wrote the Bible?\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate response\n",
    "# outputs = model.generate(input_ids, max_length=25, num_return_sequences=1,\n",
    "#                          pad_token_id=tokenizer.eos_token_id, temperature=0.7, do_sample=True)\n",
    "\n",
    "# # Decode\n",
    "# response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"distilgpt2\"\n",
    "# model_name = \"microsoft/DialoGPT-small\"\n",
    "# model_name = \"gpt2\"\n",
    "# model_name = \"google/flan-t5-base\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Retrieve local model (download on first call)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"Running on: {model.device}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb33945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: meta-llama/Llama-3.2-1B-Instruct\n",
      "Number of parameters: 1,235,814,400\n",
      "Running on: cpu \n",
      "\n",
      "Prompt: Hello! How are you?\n",
      "Bot:  I'm excited to start this new project with you! I've been thinking about how we can improve the user experience on our website.\n",
      "\n",
      "I'd like to propose a few ideas to enhance the user experience. Firstly, I think we could create a more intuitive navigation menu that makes it easier for users to find what they need. Perhaps we could add a section for frequently asked questions or a \"help\" section that provides clear instructions on how to use our website.\n",
      "\n",
      "Additionally\n",
      "\n",
      "Prompt: What's your favorite color?\n",
      "Bot:  I was thinking of implementing a new feature that would allow users to save their favorite pages or articles for later. This could be a \" favorites\" section that allows users to save content for easy access later. Do you think this is something that would be\n"
     ]
    }
   ],
   "source": [
    "# Initialize chat history\n",
    "chat_history_ids = None\n",
    "\n",
    "# First message\n",
    "prompt = \"Hello! How are you?\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "chat_history_ids = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.eos_token_id,\n",
    "                                  temperature=0.7, do_sample=True)\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "print(f\"Bot: {response}\")\n",
    "\n",
    "# Continue conversation\n",
    "print()\n",
    "prompt = \"What's your favorite color?\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "chat_history_ids = torch.cat([chat_history_ids, input_ids], dim=-1)\n",
    "\n",
    "chat_history_ids = model.generate(chat_history_ids, max_length=200, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "response = tokenizer.decode(chat_history_ids[:, -50:][0], skip_special_tokens=True)\n",
    "print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72d6eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What's your favorite color?\n",
      "Bot:  What's your favorite food? What's your favorite hobby? What's your favorite place to visit?\n",
      "Do you have any pets? What's your favorite type of music?\n",
      "\n",
      "I'd love to hear about your interests and preferences! It's always great to meet someone who shares similar tastes and passions.\n",
      "\n",
      "Also, I have to ask: Are you a morning person, a night owl, or somewhere in between?\n",
      "\n",
      "Prompt: Tell me a joke\n",
      "Bot: . Why was the math book sad?\n",
      "\n",
      "(wait for the punchline)\n",
      "\n",
      "Because it had too many problems!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat through pipeline\n",
    "chatbot = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "prompt = [\"What's your favorite color?\", \"Tell me a joke\"]\n",
    "\n",
    "# Generate response\n",
    "response = chatbot(prompt, truncation=True, num_return_sequences=1,\n",
    "                   pad_token_id=tokenizer.eos_token_id) # max_length=50\n",
    "\n",
    "for p, r in zip(prompt, response):\n",
    "    print(f\"Prompt: {p}\")\n",
    "    print(f\"Bot: {r[0]['generated_text'][len(p):]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ce059",
   "metadata": {},
   "source": [
    "### Extract json information\n",
    "Define a json schema for the output. Read a sample email in a text file. Give instruction to the model to extract information from the input email in the json format. Models suitable for instruction have been trained with a specific instruction syntax, which may differ with the model and should be followed for optimal response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "172f8242",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define json schema\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOutputSchemaModel\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[0;32m      3\u001b[0m     customer_name: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m      4\u001b[0m     phone_number: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Define json schema\n",
    "class OutputSchemaModel(BaseModel):\n",
    "    customer_name: str\n",
    "    phone_number: str\n",
    "    order_number: str\n",
    "    delivery_address: str\n",
    "\n",
    "output_schema = OutputSchemaModel.model_json_schema()\n",
    "print(output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "048f6f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Issue with Recent Order #48291\n",
      "\n",
      "From: emma.johnson@example.com\n",
      "\n",
      "To: support@shopfast.com\n",
      "\n",
      "Date: October 26, 2025\n",
      "\n",
      "Hi ShopFast team,\n",
      "\n",
      "I placed an order (Order #48291) on October 20, but the package hasn’t arrived yet at 456 Kennedy Ave, 121489 Atlanta, even though the tracking page says “Delivered” since October 23. Could you please check what happened?\n",
      "\n",
      "Also, I was charged twice for this order on my credit card. Please confirm if I’ll get a refund for the duplicate charge.\n",
      "\n",
      "Thanks,\n",
      "Emma Johnson\n",
      "+44 7911 123456\n"
     ]
    }
   ],
   "source": [
    "file = os.path.join(data_folder, \"customer_support.txt\")\n",
    "with open(file, \"r\", encoding='utf-8') as f:\n",
    "    email = f.read()\n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae8620",
   "metadata": {},
   "source": [
    "The first time AutoModelForCausalLM.from_pretrained() is called, it will download the model to the local drive, typically under C:\\Users\\YourUserName\\.cache\\huggingface\\hub\\."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dcad8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json inference\n",
    "def extract_json(text):\n",
    "    prompt = f\"Extract as JSON: {text}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "340ffbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract as JSON: John Doe is 30 years old, email john@example.com, and has a degree in Computer Science. John is married to Jane Doe, who is 28 years old, has a degree in Business, and has a degree in Psychology. John and Jane are both married and have two children, Emily (10) and Michael (7). John has a job at XYZ Corporation, which is a large and well-established company. The company has over 1,000 employees and is headquartered in New York City. John's salary is $120,000 per year. He is also a member of the New York City Police Department, which is responsible for maintaining law and order in the city. John is a member of the local community center and participates in the annual charity event for children's health and education. John has a car and drives a Honda Civic. He enjoys playing basketball and hiking in his free time. John is a big fan of the New York Yankees and attends their games whenever he can. He is a big fan of the New England Patriots and attends their games whenever he can. John is a member of the local Lions Club and participates in the annual charity event for children's health and education. John has a car and drives a Honda Civic. He enjoys playing basketball and hiking in his free time.\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Works offline!\n",
    "result = extract_json(\"John Doe is 30 years old, email john@example.com\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250ae43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311.tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
